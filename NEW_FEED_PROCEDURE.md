# üìã Proc√©dure d'Ajout d'un Nouveau Feed

## üéØ Vue d'ensemble

Ce document d√©crit la proc√©dure compl√®te pour ajouter un nouveau feed Kafka dans le Data Lake et le Data Warehouse, en r√©utilisant au maximum le travail existant.

---

## ‚ö° Proc√©dure Rapide (5 √©tapes)

```
1. Enregistrer le feed dans le registre
2. Configurer dans kafka_config.py
3. Cr√©er le topic Kafka (si n√©cessaire)
4. Red√©marrer les consumers
5. V√©rifier l'ingestion
```

**Temps estim√©**: 10-15 minutes

---

## üìù Proc√©dure D√©taill√©e

### √âtape 1: Enregistrer le Feed dans le Registre

#### 1.1 Ajouter dans la base de donn√©es

```sql
-- Se connecter √† MySQL
mysql -u root -p data_warehouse

-- Enregistrer le nouveau feed
INSERT INTO feed_registry (
    feed_name,
    feed_type,
    source_type,
    kafka_topic,
    partitioning_strategy,
    storage_format,
    compression,
    schema_definition,
    created_by
) VALUES (
    'nouveau_feed',              -- Nom du feed
    'stream',                    -- 'stream' ou 'table'
    'kafka',                     -- Source
    'nouveau_topic_kafka',       -- Topic Kafka
    'date',                      -- 'date' ou 'version'
    'parquet',                   -- Format
    'snappy',                    -- Compression
    '{"fields": ["field1", "field2"]}',  -- Sch√©ma JSON
    1                            -- ID utilisateur
);

-- V√©rifier
SELECT * FROM feed_registry WHERE feed_name = 'nouveau_feed';
```

#### 1.2 D√©finir la politique de r√©tention

```sql
-- Ajouter une politique de r√©tention
INSERT INTO data_retention_policies (
    feed_name,
    feed_type,
    retention_days,
    retention_versions,
    auto_delete,
    created_by
) VALUES (
    'nouveau_feed',
    'stream',
    90,                          -- R√©tention en jours
    NULL,                        -- NULL pour streams, nombre pour tables
    TRUE,
    1
);

-- V√©rifier
SELECT * FROM data_retention_policies WHERE feed_name = 'nouveau_feed';
```

---

### √âtape 2: Configurer dans kafka_config.py

#### 2.1 √âditer le fichier de configuration

```python
# Ouvrir kafka_config.py
vim kafka_config.py
```

#### 2.2 Ajouter le feed

**Pour un Stream** (partitionn√© par date):

```python
KAFKA_TOPICS = {
    "streams": [
        # ... feeds existants ...
        
        # NOUVEAU FEED
        {
            "topic": "nouveau_topic_kafka",
            "feed_type": "stream",
            "destination": "data_lake",  # ou "both" pour DL + DW
            "partitioning": "date",
            "storage_mode": "append",
            "enabled": True
        }
    ],
    # ...
}
```

**Pour une Table** (partitionn√©e par version):

```python
KAFKA_TOPICS = {
    # ...
    "tables": [
        # ... feeds existants ...
        
        # NOUVEAU FEED
        {
            "topic": "nouveau_topic_kafka",
            "feed_type": "table",
            "destination": "both",  # Data Lake + Data Warehouse
            "partitioning": "version",
            "storage_mode": "overwrite",
            "mysql_table": "fact_nouveau_feed",  # Table MySQL
            "enabled": True
        }
    ]
}
```

#### 2.3 Ajouter le sch√©ma (optionnel)

```python
MESSAGE_SCHEMAS = {
    # ... sch√©mas existants ...
    
    "nouveau_topic_kafka": {
        "fields": ["field1", "field2", "field3", "timestamp"],
        "key_field": "field1"
    }
}
```

---

### √âtape 3: Cr√©er le Topic Kafka

#### 3.1 Cr√©er le topic

```bash
# Cr√©er le topic Kafka
kafka-topics --bootstrap-server localhost:9092 \
  --create \
  --topic nouveau_topic_kafka \
  --partitions 3 \
  --replication-factor 1

# V√©rifier
kafka-topics --bootstrap-server localhost:9092 --list | grep nouveau
```

#### 3.2 Configurer les param√®tres (optionnel)

```bash
# D√©finir la r√©tention Kafka (7 jours)
kafka-configs --bootstrap-server localhost:9092 \
  --entity-type topics \
  --entity-name nouveau_topic_kafka \
  --alter \
  --add-config retention.ms=604800000

# D√©finir la compression
kafka-configs --bootstrap-server localhost:9092 \
  --entity-type topics \
  --entity-name nouveau_topic_kafka \
  --alter \
  --add-config compression.type=snappy
```

---

### √âtape 4: Cr√©er la Table MySQL (si destination = "both")

#### 4.1 Cr√©er le script SQL

```bash
# Cr√©er le fichier SQL
vim sql/06_create_nouveau_feed_table.sql
```

#### 4.2 D√©finir le sch√©ma

```sql
USE data_warehouse;

-- Table pour le nouveau feed
CREATE TABLE IF NOT EXISTS fact_nouveau_feed (
    id INT AUTO_INCREMENT PRIMARY KEY,
    field1 VARCHAR(255) NOT NULL,
    field2 DECIMAL(15,2),
    field3 VARCHAR(100),
    snapshot_date DATE NOT NULL,
    snapshot_version INT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    INDEX idx_field1 (field1),
    INDEX idx_snapshot_date (snapshot_date),
    INDEX idx_snapshot_version (snapshot_version),
    
    UNIQUE KEY unique_snapshot (field1, snapshot_date, snapshot_version)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
COMMENT='Nouveau feed depuis Kafka';

SELECT 'Table fact_nouveau_feed cr√©√©e' AS Status;
```

#### 4.3 Ex√©cuter le script

```bash
mysql -u root -p < sql/06_create_nouveau_feed_table.sql
```

---

### √âtape 5: Adapter le Consumer Warehouse (si n√©cessaire)

#### 5.1 Ajouter la logique d'insertion

Si le feed va dans le Data Warehouse, ajouter la m√©thode d'insertion dans `kafka_consumer_warehouse.py`:

```python
def insert_nouveau_feed(self, df: pd.DataFrame):
    """Ins√®re les donn√©es du nouveau feed"""
    query = """
    INSERT INTO fact_nouveau_feed (
        field1, field2, field3, snapshot_date, snapshot_version
    ) VALUES (
        %(field1)s, %(field2)s, %(field3)s, %(snapshot_date)s, %(snapshot_version)s
    )
    ON DUPLICATE KEY UPDATE
        field2 = VALUES(field2),
        field3 = VALUES(field3),
        updated_at = CURRENT_TIMESTAMP
    """
    
    for _, row in df.iterrows():
        data = {
            "field1": row.get("field1"),
            "field2": row.get("field2"),
            "field3": row.get("field3"),
            "snapshot_date": self.snapshot_date,
            "snapshot_version": self.snapshot_version
        }
        self.mysql_cursor.execute(query, data)
    
    self.mysql_connection.commit()
```

#### 5.2 Ajouter dans flush_buffer()

```python
def flush_buffer(self, topic: str):
    # ... code existant ...
    
    # Ajouter le nouveau feed
    elif topic == "nouveau_topic_kafka":
        self.insert_nouveau_feed(df)
    else:
        logger.warning(f"Topic non support√©: {topic}")
```

---

### √âtape 6: Red√©marrer les Consumers

#### 6.1 Arr√™ter les consumers existants

```bash
# Si les consumers tournent
ps aux | grep kafka_consumer

# Arr√™ter avec Ctrl+C ou
kill <PID>
```

#### 6.2 Red√©marrer

```bash
# Activer l'environnement virtuel
source venv/bin/activate

# Red√©marrer tous les consumers
python kafka_consumer_orchestrator.py --mode all --mysql-password ""

# Ou uniquement Data Lake
python kafka_consumer_datalake.py

# Ou uniquement Data Warehouse
python kafka_consumer_warehouse.py --mysql-password ""
```

---

### √âtape 7: V√©rifier l'Ingestion

#### 7.1 V√©rifier les logs

```bash
# Logs du consumer Data Lake
tail -f data_lake/logs/kafka_consumer_datalake_*.log

# Logs du consumer Data Warehouse
tail -f data_lake/logs/kafka_consumer_warehouse_*.log
```

#### 7.2 V√©rifier le Data Lake

```bash
# V√©rifier que les fichiers sont cr√©√©s
ls -lh data_lake/streams/nouveau_feed/
# ou
ls -lh data_lake/tables/nouveau_feed/

# Lire un fichier Parquet
python -c "
import pandas as pd
df = pd.read_parquet('data_lake/streams/nouveau_feed/year=2025/month=01/day=28/data_*.parquet')
print(df.head())
print(f'Lignes: {len(df)}')
"
```

#### 7.3 V√©rifier le Data Warehouse

```bash
# V√©rifier les donn√©es dans MySQL
mysql -u root -p -e "
USE data_warehouse;
SELECT COUNT(*) as count FROM fact_nouveau_feed;
SELECT * FROM fact_nouveau_feed LIMIT 10;
"
```

#### 7.4 Tester l'envoi de messages

```bash
# Envoyer un message de test
echo '{"field1":"test","field2":123.45,"field3":"value"}' | \
kafka-console-producer --bootstrap-server localhost:9092 \
  --topic nouveau_topic_kafka

# Attendre quelques secondes et v√©rifier
```

---

## üîÑ R√©utilisation du Travail Existant

### ‚úÖ Ce qui est Automatiquement R√©utilis√©

#### 1. Infrastructure Kafka
- ‚úÖ Connexion Kafka d√©j√† configur√©e
- ‚úÖ Gestion des offsets
- ‚úÖ Batch processing (1000 messages)
- ‚úÖ Gestion d'erreurs

#### 2. √âcriture Parquet
- ‚úÖ Conversion DataFrame ‚Üí Parquet
- ‚úÖ Compression Snappy
- ‚úÖ M√©tadonn√©es automatiques
- ‚úÖ Partitionnement (date/version)

#### 3. Gestion MySQL
- ‚úÖ Connexion pool
- ‚úÖ Transactions ACID
- ‚úÖ UPSERT automatique
- ‚úÖ Gestion des snapshots

#### 4. Monitoring
- ‚úÖ Logs structur√©s
- ‚úÖ M√©triques de performance
- ‚úÖ Alertes d'erreurs

#### 5. Gouvernance
- ‚úÖ Enregistrement dans feed_registry
- ‚úÖ Politique de r√©tention
- ‚úÖ Journal d'audit
- ‚úÖ Permissions

---

### üöÄ Avantages de la R√©utilisation

| Aspect | Sans R√©utilisation | Avec R√©utilisation | Gain |
|--------|-------------------|-------------------|------|
| **Temps de d√©veloppement** | 2-3 jours | 15 minutes | 95% |
| **Code √† √©crire** | ~500 lignes | ~50 lignes | 90% |
| **Tests** | Complets | Minimaux | 80% |
| **Documentation** | Compl√®te | Mise √† jour | 70% |
| **D√©ploiement** | Configuration compl√®te | Red√©marrage | 90% |

---

## üìä Checklist de Validation

### Avant le D√©ploiement
- [ ] Feed enregistr√© dans `feed_registry`
- [ ] Politique de r√©tention d√©finie
- [ ] Configuration ajout√©e dans `kafka_config.py`
- [ ] Topic Kafka cr√©√©
- [ ] Table MySQL cr√©√©e (si n√©cessaire)
- [ ] Logique d'insertion ajout√©e (si n√©cessaire)
- [ ] Tests unitaires pass√©s

### Apr√®s le D√©ploiement
- [ ] Consumers red√©marr√©s sans erreur
- [ ] Messages consomm√©s (v√©rifier logs)
- [ ] Fichiers Parquet cr√©√©s
- [ ] Donn√©es dans MySQL (si applicable)
- [ ] Permissions configur√©es
- [ ] Monitoring actif
- [ ] Documentation mise √† jour

---

## üêõ Troubleshooting

### Probl√®me 1: Topic non trouv√©

```bash
# Erreur: Topic nouveau_topic_kafka is not available

# Solution: Cr√©er le topic
kafka-topics --bootstrap-server localhost:9092 \
  --create --topic nouveau_topic_kafka \
  --partitions 3 --replication-factor 1
```

### Probl√®me 2: Consumer ne d√©marre pas

```bash
# Erreur: Topic non support√©

# Solution: V√©rifier kafka_config.py
grep "nouveau_topic_kafka" kafka_config.py

# Red√©marrer les consumers
python kafka_consumer_orchestrator.py --mode all --mysql-password ""
```

### Probl√®me 3: Erreur MySQL

```bash
# Erreur: Table doesn't exist

# Solution: Cr√©er la table
mysql -u root -p < sql/06_create_nouveau_feed_table.sql
```

### Probl√®me 4: Pas de donn√©es

```bash
# V√©rifier que le producer envoie des messages
kafka-console-consumer --bootstrap-server localhost:9092 \
  --topic nouveau_topic_kafka \
  --from-beginning \
  --max-messages 10
```

---

## üìà Exemple Complet

### Ajout d'un Feed "customer_events"

```bash
# 1. Enregistrer dans MySQL
mysql -u root -p data_warehouse << EOF
INSERT INTO feed_registry (feed_name, feed_type, source_type, kafka_topic, partitioning_strategy, created_by)
VALUES ('customer_events', 'stream', 'kafka', 'customer_events', 'date', 1);

INSERT INTO data_retention_policies (feed_name, feed_type, retention_days, created_by)
VALUES ('customer_events', 'stream', 180, 1);
EOF

# 2. Ajouter dans kafka_config.py
cat >> kafka_config.py << 'EOF'
# Dans KAFKA_TOPICS["streams"]:
{
    "topic": "customer_events",
    "feed_type": "stream",
    "destination": "data_lake",
    "partitioning": "date",
    "storage_mode": "append",
    "enabled": True
}
EOF

# 3. Cr√©er le topic Kafka
kafka-topics --bootstrap-server localhost:9092 \
  --create --topic customer_events \
  --partitions 3 --replication-factor 1

# 4. Red√©marrer les consumers
python kafka_consumer_orchestrator.py --mode all --mysql-password ""

# 5. Tester
echo '{"customer_id":"C123","event":"login","timestamp":"2025-01-28T20:00:00Z"}' | \
kafka-console-producer --bootstrap-server localhost:9092 --topic customer_events

# 6. V√©rifier
ls -lh data_lake/streams/customer_events/
```

**Temps total: 10 minutes** ‚ö°

---

## üéØ R√©sum√©

### Travail N√©cessaire
- ‚úÖ Configuration: 5 minutes
- ‚úÖ Cr√©ation topic: 1 minute
- ‚úÖ Table MySQL (optionnel): 5 minutes
- ‚úÖ Tests: 5 minutes

### Travail R√©utilis√© Automatiquement
- ‚úÖ Consumer Kafka (~300 lignes)
- ‚úÖ √âcriture Parquet (~200 lignes)
- ‚úÖ Gestion MySQL (~300 lignes)
- ‚úÖ Monitoring (~100 lignes)
- ‚úÖ Gouvernance (~200 lignes)

**Total r√©utilis√©: ~1100 lignes de code !** üöÄ

---

**Version**: 1.0  
**Date**: Janvier 2025  
**Statut**: ‚úÖ Proc√©dure Valid√©e
