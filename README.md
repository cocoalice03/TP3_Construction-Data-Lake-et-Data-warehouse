# ğŸï¸ Data Lake - Stockage et Gestion des DonnÃ©es ksqlDB

> Solution complÃ¨te de data lake pour stocker, partitionner et gÃ©rer les donnÃ©es provenant de ksqlDB

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![Parquet](https://img.shields.io/badge/Format-Parquet-green)](https://parquet.apache.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow)](LICENSE)

---

## ğŸ“‹ Vue d'ensemble

Ce data lake offre une solution robuste et extensible pour:
- âœ… **Stocker** les donnÃ©es ksqlDB (streams et tables) sur le systÃ¨me de fichiers
- âœ… **Partitionner** intelligemment par date (streams) et version (tables)
- âœ… **GÃ©rer** facilement l'ajout de nouveaux feeds
- âœ… **Optimiser** les performances avec le format Parquet
- âœ… **Tracer** toutes les opÃ©rations avec mÃ©tadonnÃ©es et logs

---

## ğŸš€ DÃ©marrage Rapide

### Installation

```bash
# 1. Cloner le projet
cd /Users/alice/Downloads/data_lake

# 2. Installer les dÃ©pendances
pip install -r requirements.txt

# 3. Valider l'installation
python test_setup.py

# 4. CrÃ©er la structure du data lake
python data_lake_config.py

# 5. Synchroniser les feeds
python manage_feeds.py sync
```

### Premier Export

```bash
# Configurer ksqlDB dans data_lake_config.py
# Puis exporter toutes les donnÃ©es
python export_to_data_lake.py --all
```

---

## ğŸ“ Structure du Projet

```
data_lake/
â”œâ”€â”€ ğŸ“„ DESIGN_DOCUMENT.md          # Design complet avec justifications
â”œâ”€â”€ ğŸ“„ ARCHITECTURE.md             # Architecture technique
â”œâ”€â”€ ğŸ“„ QUICKSTART.md               # Guide de dÃ©marrage rapide
â”œâ”€â”€ ğŸ“„ SUMMARY.md                  # RÃ©sumÃ© du projet
â”œâ”€â”€ ğŸ“„ README.md                   # Ce fichier
â”‚
â”œâ”€â”€ ğŸ data_lake_config.py         # Configuration centralisÃ©e
â”œâ”€â”€ ğŸ export_to_data_lake.py      # Script d'export principal
â”œâ”€â”€ ğŸ manage_feeds.py             # Gestion des feeds
â”œâ”€â”€ ğŸ metadata_utils.py           # Utilitaires mÃ©tadonnÃ©es
â”œâ”€â”€ ğŸ test_setup.py               # Tests de validation
â”‚
â”œâ”€â”€ ğŸ“¦ requirements.txt            # DÃ©pendances Python
â”œâ”€â”€ ğŸš« .gitignore                  # Fichiers Ã  ignorer
â”‚
â””â”€â”€ ğŸ“‚ data_lake/                  # Dossier du data lake
    â”œâ”€â”€ streams/                   # DonnÃ©es des streams
    â”œâ”€â”€ tables/                    # DonnÃ©es des tables
    â”œâ”€â”€ feeds/                     # Configuration des feeds
    â”‚   â”œâ”€â”€ active/
    â”‚   â””â”€â”€ archived/
    â”œâ”€â”€ logs/                      # Logs d'export
    â””â”€â”€ README.md                  # Documentation dÃ©taillÃ©e
```

---

## ğŸ¯ FonctionnalitÃ©s Principales

### 1. Export des DonnÃ©es

```bash
# Export complet (tous les streams et tables)
python export_to_data_lake.py --all

# Export d'un stream spÃ©cifique
python export_to_data_lake.py --stream transaction_stream_anonymized

# Export d'une table spÃ©cifique
python export_to_data_lake.py --table user_transaction_summary

# Export avec date spÃ©cifique
python export_to_data_lake.py --stream transaction_stream --date 2025-01-15
```

### 2. Gestion des Feeds

```bash
# Lister les feeds
python manage_feeds.py list

# Ajouter un nouveau feed
python manage_feeds.py add \
  --name payment_stream \
  --type stream \
  --source payment_stream \
  --description "Stream des paiements"

# DÃ©sactiver un feed
python manage_feeds.py disable transaction_stream

# Archiver un feed
python manage_feeds.py archive old_stream

# Synchroniser depuis la configuration
python manage_feeds.py sync
```

### 3. Monitoring et MÃ©tadonnÃ©es

```bash
# Afficher les statistiques
python metadata_utils.py --stats

# GÃ©nÃ©rer un rapport complet
python metadata_utils.py --report

# Exporter en CSV
python metadata_utils.py --csv data_lake_report.csv
```

---

## ğŸ“Š Architecture

### Partitionnement

#### Streams (Mode APPEND)
```
streams/transaction_stream/
â””â”€â”€ year=2025/
    â””â”€â”€ month=01/
        â””â”€â”€ day=15/
            â”œâ”€â”€ data_20250115_120000.parquet
            â”œâ”€â”€ data_20250115_140000.parquet
            â””â”€â”€ data_20250115_160000.parquet
```

**Pourquoi APPEND?**
- Ã‰vÃ©nements immuables (Event Sourcing)
- Historique complet pour audit
- Replay des donnÃ©es possible

#### Tables (Mode OVERWRITE)
```
tables/user_transaction_summary/
â”œâ”€â”€ version=v1/
â”‚   â””â”€â”€ snapshot_20250115_120000.parquet
â”œâ”€â”€ version=v2/
â”‚   â””â”€â”€ snapshot_20250116_120000.parquet
â””â”€â”€ version=v3/
    â””â”€â”€ snapshot_20250117_120000.parquet
```

**Pourquoi OVERWRITE?**
- Snapshots complets de l'Ã©tat actuel
- Pas de duplication de donnÃ©es agrÃ©gÃ©es
- RÃ©tention configurable (7 versions par dÃ©faut)

### Format Parquet

**Avantages:**
- ğŸš€ Compression 70-90% vs JSON
- ğŸ“Š Format columnaire pour l'analytique
- ğŸ”§ Compatible Pandas, Spark, DuckDB
- ğŸ“ SchÃ©ma intÃ©grÃ©
- ğŸ¯ Partitionnement natif

---

## ğŸ”„ Ajout de Nouveaux Feeds

### MÃ©thode 1: Via Script

```bash
python manage_feeds.py add \
  --name new_stream \
  --type stream \
  --source new_stream_ksqldb \
  --description "Description du nouveau stream" \
  --partitioning date \
  --storage-mode append \
  --retention-days 365
```

### MÃ©thode 2: Configuration JSON

CrÃ©er `data_lake/feeds/active/new_stream.json`:

```json
{
  "feed_name": "new_stream",
  "feed_type": "stream",
  "ksqldb_source": "new_stream_ksqldb",
  "description": "Description du nouveau stream",
  "partitioning": {
    "type": "date",
    "columns": ["year", "month", "day"]
  },
  "storage_mode": "append",
  "format": "parquet",
  "retention_days": 365,
  "enabled": true,
  "created_at": "2025-01-15T10:00:00Z"
}
```

Le feed sera automatiquement dÃ©tectÃ© lors du prochain export.

---

## ğŸ“– Lecture des DonnÃ©es

### Avec Pandas

```python
import pandas as pd

# Lire un stream avec filtre par date
df = pd.read_parquet(
    'data_lake/streams/transaction_stream_anonymized',
    filters=[('year', '=', 2025), ('month', '=', 1)]
)

# Lire une table (derniÃ¨re version)
df = pd.read_parquet(
    'data_lake/tables/user_transaction_summary/version=v3'
)
```

### Avec DuckDB

```python
import duckdb

# RequÃªte SQL sur le data lake
result = duckdb.sql("""
    SELECT 
        user_id,
        SUM(amount) as total_amount
    FROM 'data_lake/streams/transaction_stream_anonymized/**/*.parquet'
    WHERE year = 2025 AND month = 1
    GROUP BY user_id
""").df()
```

---

## ğŸ” SÃ©curitÃ© et ConformitÃ©

### Niveaux de SÃ©curitÃ©

1. **DonnÃ©es brutes** - AccÃ¨s restreint
   ```bash
   chmod 750 data_lake/streams/transaction_stream/
   ```

2. **DonnÃ©es anonymisÃ©es** - ConformitÃ© RGPD
   ```bash
   chmod 755 data_lake/streams/transaction_stream_anonymized/
   ```

3. **AgrÃ©gations** - AccÃ¨s Ã©tendu
   ```bash
   chmod 755 data_lake/tables/
   ```

---

## ğŸ“ˆ Streams et Tables ConfigurÃ©s

### Streams (4)

| Stream | Description | RÃ©tention |
|--------|-------------|-----------|
| `transaction_stream` | DonnÃ©es brutes | 365 jours |
| `transaction_flattened` | SchÃ©ma aplati | 365 jours |
| `transaction_stream_anonymized` | AnonymisÃ© + EUR | 730 jours |
| `transaction_stream_blacklisted` | Villes blacklistÃ©es | 365 jours |

### Tables (4)

| Table | Description | Versions |
|-------|-------------|----------|
| `user_transaction_summary` | Montants par utilisateur | 7 |
| `user_transaction_summary_eur` | Montants en EUR | 7 |
| `payment_method_totals` | Totaux par mÃ©thode | 7 |
| `product_purchase_counts` | Compteurs par produit | 7 |

---

## ğŸ”§ Configuration

### ksqlDB

Modifier dans `data_lake_config.py`:

```python
KSQLDB_CONFIG = {
    "host": "localhost",
    "port": 8088,
    "timeout": 30
}
```

### Parquet

```python
STORAGE_FORMAT = "parquet"
PARQUET_COMPRESSION = "snappy"
BATCH_SIZE = 10000
```

---

## ğŸ¤– Automatisation

### Cron Job (Export Quotidien)

```bash
# Ajouter au crontab
crontab -e

# Export quotidien Ã  2h du matin
0 2 * * * cd /Users/alice/Downloads/data_lake && python export_to_data_lake.py --all >> data_lake/logs/cron.log 2>&1
```

---

## ğŸ“Š MÃ©tadonnÃ©es

Chaque stream/table contient `_metadata.json`:

```json
{
  "source": "transaction_stream",
  "type": "stream",
  "storage_mode": "append",
  "format": "parquet",
  "partitioning": "date",
  "last_export": "2025-01-15T14:30:00Z",
  "total_records": 150000,
  "total_size_mb": 45.2,
  "partitions": [...]
}
```

---

## ğŸ§ª Tests

```bash
# Valider l'installation complÃ¨te
python test_setup.py

# RÃ©sultat attendu:
# âœ“ Installation validÃ©e avec succÃ¨s!
# 7/7 tests rÃ©ussis
```

---

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| **DESIGN_DOCUMENT.md** | Design complet avec justifications techniques |
| **ARCHITECTURE.md** | Diagrammes et architecture dÃ©taillÃ©e |
| **QUICKSTART.md** | Guide de dÃ©marrage rapide |
| **SUMMARY.md** | RÃ©sumÃ© exÃ©cutif du projet |
| **data_lake/README.md** | Documentation utilisateur dÃ©taillÃ©e |

---

## ğŸ”® Roadmap

### Phase 1: MVP âœ…
- [x] Export local
- [x] Partitionnement date/version
- [x] Format Parquet
- [x] Gestion des feeds

### Phase 2: Cloud
- [ ] Export vers S3/Azure Blob
- [ ] Tiering de stockage
- [ ] Catalogue de donnÃ©es

### Phase 3: Advanced
- [ ] Delta Lake (ACID)
- [ ] Time travel queries
- [ ] Streaming temps rÃ©el
- [ ] Data quality checks

---

## ğŸ› Troubleshooting

### Erreur de connexion ksqlDB

```bash
# VÃ©rifier que ksqlDB est accessible
curl http://localhost:8088/info
```

### Permissions insuffisantes

```bash
# Donner les permissions
chmod -R 755 data_lake/
```

### Consulter les logs

```bash
# Logs d'export
tail -f data_lake/logs/export_*.log
```

---

## ğŸ¤ Support

Pour toute question:
1. Consulter la documentation dans `DESIGN_DOCUMENT.md`
2. VÃ©rifier les exemples dans `QUICKSTART.md`
3. Examiner les logs dans `data_lake/logs/`

---

## ğŸ“ License

MIT License - Voir LICENSE pour plus de dÃ©tails

---

**Version**: 1.0  
**DerniÃ¨re mise Ã  jour**: Janvier 2025  
**Statut**: âœ… Production Ready
