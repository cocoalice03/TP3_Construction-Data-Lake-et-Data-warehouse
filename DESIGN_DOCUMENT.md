# üèóÔ∏è Document de Design - Data Lake

## üìã Vue d'ensemble

Ce document d√©crit l'architecture compl√®te du Data Lake con√ßu pour stocker et g√©rer les donn√©es provenant de ksqlDB. Le data lake est impl√©ment√© sur le syst√®me de fichiers local avec un partitionnement intelligent et une gestion automatis√©e des feeds.

---

## üéØ Objectifs du Design

1. **Stockage structur√©** des donn√©es ksqlDB (streams et tables)
2. **Partitionnement optimis√©** par date pour les streams et par version pour les tables
3. **Extensibilit√©** pour l'ajout de nouveaux feeds sans modification du code
4. **Tra√ßabilit√©** compl√®te avec m√©tadonn√©es et logs
5. **Performance** optimale pour les requ√™tes analytiques

---

## üóÇÔ∏è Architecture du Data Lake

### Structure des Dossiers

```
data_lake/
‚îú‚îÄ‚îÄ streams/                    # Donn√©es des streams (√©v√©nements immuables)
‚îÇ   ‚îú‚îÄ‚îÄ transaction_stream/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ year=2025/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ month=01/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ day=15/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_20250115_*.parquet
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ day=16/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ _metadata.json
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ transaction_flattened/
‚îÇ   ‚îú‚îÄ‚îÄ transaction_stream_anonymized/
‚îÇ   ‚îî‚îÄ‚îÄ transaction_stream_blacklisted/
‚îÇ
‚îú‚îÄ‚îÄ tables/                     # Donn√©es des tables (agr√©gations)
‚îÇ   ‚îú‚îÄ‚îÄ user_transaction_summary/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ version=v1/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ snapshot_20250115_*.parquet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ version=v2/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ _metadata.json
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ user_transaction_summary_eur/
‚îÇ   ‚îú‚îÄ‚îÄ payment_method_totals/
‚îÇ   ‚îî‚îÄ‚îÄ product_purchase_counts/
‚îÇ
‚îú‚îÄ‚îÄ feeds/                      # Configuration des feeds
‚îÇ   ‚îú‚îÄ‚îÄ active/                 # Feeds actifs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transaction_stream.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transaction_stream_anonymized.json
‚îÇ   ‚îî‚îÄ‚îÄ archived/               # Feeds archiv√©s
‚îÇ
‚îî‚îÄ‚îÄ logs/                       # Logs d'export
    ‚îî‚îÄ‚îÄ export_2025_01.log
```

---

## üìä Strat√©gie de Partitionnement

### Partitionnement par Date (Streams)

**Utilis√© pour**: Tous les streams de transactions

**Structure**: `year=YYYY/month=MM/day=DD/`

**Avantages**:
- ‚úÖ **Requ√™tes rapides** filtr√©es par p√©riode
- ‚úÖ **Maintenance facile** (suppression des anciennes partitions)
- ‚úÖ **Scalabilit√©** (ajout sans impact sur l'existant)
- ‚úÖ **Parall√©lisme** (lecture/√©criture parall√®le)
- ‚úÖ **Archivage s√©lectif** des anciennes donn√©es

**Exemple de requ√™te optimis√©e**:
```python
# Lire uniquement janvier 2025
df = pd.read_parquet(
    'data_lake/streams/transaction_stream',
    filters=[('year', '=', 2025), ('month', '=', 1)]
)
```

### Partitionnement par Version (Tables)

**Utilis√© pour**: Toutes les tables d'agr√©gation

**Structure**: `version=vX/`

**Avantages**:
- ‚úÖ **Snapshots complets** de l'√©tat √† un instant T
- ‚úÖ **Historique des versions** pour audit
- ‚úÖ **Rollback facile** en cas de probl√®me
- ‚úÖ **Comparaison** entre versions
- ‚úÖ **R√©tention configurable** (garder N derni√®res versions)

**Exemple**:
```
version=v1/  ‚Üí Snapshot du 15/01/2025
version=v2/  ‚Üí Snapshot du 16/01/2025
version=v3/  ‚Üí Snapshot du 17/01/2025
```

---

## üíæ Modes de Stockage

### Mode APPEND (Streams)

**Principe**: Ajouter sans modifier l'existant

**Justification**:
- Les streams repr√©sentent des **√©v√©nements immuables**
- Chaque transaction est **unique** et ne doit jamais √™tre modifi√©e
- Permet de **conserver l'historique complet** pour audit
- Facilite le **replay** des donn√©es
- Optimise les **performances d'√©criture**
- Compatible avec **Event Sourcing**

**Impl√©mentation**:
```python
# Chaque export cr√©e un nouveau fichier
file_path = partition_path / f"data_{timestamp}.parquet"
pq.write_table(table, file_path)
```

### Mode OVERWRITE (Tables)

**Principe**: Remplacer compl√®tement la version

**Justification**:
- Les tables contiennent des **agr√©gations calcul√©es**
- Chaque export = **snapshot complet** de l'√©tat actuel
- √âvite la **duplication** de donn√©es agr√©g√©es
- Simplifie les **requ√™tes analytiques**
- R√©duit l'**espace disque**
- Facilite la **maintenance**

**Impl√©mentation**:
```python
# Nouvelle version √† chaque export
version = get_next_version()
partition_path = base_path / f"version=v{version}"
pq.write_table(table, partition_path / f"snapshot_{timestamp}.parquet")
```

### Mode IGNORE (Non utilis√©)

**Pourquoi pas utilis√©**:
- ‚ùå Ne convient pas aux streams (besoin d'ajouter continuellement)
- ‚ùå Ne convient pas aux tables (besoin de mettre √† jour)
- ‚ö†Ô∏è Pourrait √™tre utilis√© pour des **donn√©es de r√©f√©rence statiques**

---

## üìÅ Format de Stockage: Apache Parquet

### Choix du Format

**Format s√©lectionn√©**: Apache Parquet avec compression Snappy

### Justifications Techniques

1. **Compression efficace**
   - R√©duction de 70-90% vs JSON/CSV
   - Compression Snappy: bon compromis vitesse/taille

2. **Format columnaire**
   - Lecture optimis√©e pour l'analytique
   - Lecture s√©lective des colonnes n√©cessaires
   - Agr√©gations ultra-rapides

3. **Sch√©ma int√©gr√©**
   - Pas besoin de fichier de sch√©ma s√©par√©
   - Validation automatique des types
   - √âvolution du sch√©ma facilit√©e

4. **Compatibilit√©**
   - Pandas, Polars, DuckDB
   - Apache Spark, Apache Arrow
   - AWS Athena, Google BigQuery

5. **Partitionnement natif**
   - Support des partitions Hive-style
   - Predicate pushdown automatique

6. **Types complexes**
   - STRUCT, ARRAY, MAP support√©s
   - Nested data structures

### Configuration Parquet

```python
pq.write_table(
    table,
    file_path,
    compression='snappy',      # Compression rapide
    use_dictionary=True,       # Compression des valeurs r√©p√©t√©es
    write_statistics=True      # Stats pour predicate pushdown
)
```

---

## üîÑ Gestion des Nouveaux Feeds

### Processus d'Ajout

1. **Cr√©er la configuration** du feed (JSON)
2. **D√©tection automatique** par les scripts
3. **Cr√©ation automatique** de la structure de dossiers
4. **Export des donn√©es** selon la configuration

### Configuration d'un Feed

```json
{
  "feed_name": "payment_stream",
  "feed_type": "stream",
  "ksqldb_source": "payment_stream",
  "description": "Stream des paiements",
  "partitioning": {
    "type": "date",
    "columns": ["year", "month", "day"]
  },
  "storage_mode": "append",
  "format": "parquet",
  "retention_days": 365,
  "enabled": true,
  "created_at": "2025-01-15T10:00:00Z"
}
```

### Commandes de Gestion

```bash
# Ajouter un nouveau feed
python manage_feeds.py add \
  --name payment_stream \
  --type stream \
  --source payment_stream \
  --description "Stream des paiements"

# Lister les feeds
python manage_feeds.py list

# D√©sactiver un feed
python manage_feeds.py disable payment_stream

# Archiver un feed
python manage_feeds.py archive payment_stream
```

---

## üîê S√©curit√© et Conformit√©

### Donn√©es Anonymis√©es

- **Stream anonymis√©**: `transaction_stream_anonymized`
- **Conformit√© RGPD**: R√©tention de 2 ans
- **Donn√©es sensibles**: Isol√©es dans des streams s√©par√©s

### Permissions Recommand√©es

```bash
# Streams bruts: acc√®s restreint
chmod 750 data_lake/streams/transaction_stream/

# Streams anonymis√©s: acc√®s √©tendu
chmod 755 data_lake/streams/transaction_stream_anonymized/

# Tables: acc√®s √©tendu
chmod 755 data_lake/tables/
```

### Isolation des Donn√©es

- **Blacklist**: Stream s√©par√© `transaction_stream_blacklisted`
- **Audit trail**: Logs complets de tous les exports
- **M√©tadonn√©es**: Tra√ßabilit√© compl√®te

---

## üìà M√©tadonn√©es et Monitoring

### Fichier _metadata.json

Chaque stream/table contient un fichier de m√©tadonn√©es:

```json
{
  "source": "transaction_stream",
  "type": "stream",
  "storage_mode": "append",
  "format": "parquet",
  "partitioning": "date",
  "last_export": "2025-01-15T14:30:00Z",
  "total_records": 150000,
  "total_size_mb": 45.2,
  "partitions": [
    {
      "path": "year=2025/month=01/day=15",
      "records": 5000,
      "size_mb": 1.5,
      "exported_at": "2025-01-15T14:30:00Z"
    }
  ],
  "schema_version": "1.0",
  "created_at": "2025-01-01T00:00:00Z"
}
```

### Utilitaires de Monitoring

```bash
# G√©n√©rer un rapport complet
python metadata_utils.py --report

# Afficher les statistiques
python metadata_utils.py --stats

# Exporter en CSV
python metadata_utils.py --csv data_lake_report.csv
```

---

## üöÄ Utilisation

### Installation

```bash
# Installer les d√©pendances
pip install -r requirements.txt

# Cr√©er la structure du data lake
python data_lake_config.py

# Synchroniser les feeds depuis la configuration
python manage_feeds.py sync
```

### Export des Donn√©es

```bash
# Exporter tous les streams et tables
python export_to_data_lake.py --all

# Exporter un stream sp√©cifique
python export_to_data_lake.py --stream transaction_stream_anonymized

# Exporter une table sp√©cifique
python export_to_data_lake.py --table user_transaction_summary

# Exporter avec une date sp√©cifique
python export_to_data_lake.py --stream transaction_stream --date 2025-01-15
```

### Export Automatique (Cron)

```bash
# Ajouter au crontab pour export quotidien √† 2h du matin
0 2 * * * cd /path/to/project && python export_to_data_lake.py --all >> logs/export.log 2>&1
```

---

## üìä Mapping Streams/Tables ksqlDB

### Streams Configur√©s

| Stream ksqlDB | Type | Partitionnement | Mode | R√©tention |
|---------------|------|-----------------|------|-----------|
| `transaction_stream` | Stream | Date | APPEND | 365 jours |
| `transaction_flattened` | Stream | Date | APPEND | 365 jours |
| `transaction_stream_anonymized` | Stream | Date | APPEND | 730 jours |
| `transaction_stream_blacklisted` | Stream | Date | APPEND | 365 jours |

### Tables Configur√©es

| Table ksqlDB | Type | Partitionnement | Mode | R√©tention |
|--------------|------|-----------------|------|-----------|
| `user_transaction_summary` | Table | Version | OVERWRITE | 7 versions |
| `user_transaction_summary_eur` | Table | Version | OVERWRITE | 7 versions |
| `payment_method_totals` | Table | Version | OVERWRITE | 7 versions |
| `product_purchase_counts` | Table | Version | OVERWRITE | 7 versions |

---

## üîß Configuration Technique

### Configuration ksqlDB

```python
KSQLDB_CONFIG = {
    "host": "localhost",
    "port": 8088,
    "timeout": 30
}
```

### Configuration Parquet

```python
STORAGE_FORMAT = "parquet"
PARQUET_COMPRESSION = "snappy"
BATCH_SIZE = 10000
```

### Configuration des Logs

```python
LOG_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
LOG_LEVEL = "INFO"
```

---

## üéØ √âvolution Future

### Extensions Possibles

1. **Compression avanc√©e**
   - Tester ZSTD pour meilleure compression
   - Benchmark Snappy vs LZ4 vs ZSTD

2. **Tiering de stockage**
   - D√©placer anciennes donn√©es vers S3/Azure Blob
   - Impl√©mentation d'une politique de lifecycle

3. **Indexation**
   - Cr√©er des index pour acc√©l√©rer les recherches
   - Bloom filters pour les lookups

4. **Catalogue de donn√©es**
   - Int√©gration avec Apache Hive Metastore
   - Support AWS Glue Data Catalog

5. **Qualit√© des donn√©es**
   - Validations automatiques
   - Data quality checks
   - Alertes sur anomalies

6. **Versioning avanc√©**
   - Impl√©mentation Delta Lake
   - Time travel queries
   - ACID transactions

7. **Streaming en temps r√©el**
   - Int√©gration Apache Kafka
   - Micro-batching pour latence r√©duite

---

## üìö Fichiers du Projet

### Scripts Principaux

- **`data_lake_config.py`** - Configuration centralis√©e
- **`export_to_data_lake.py`** - Script d'export principal
- **`manage_feeds.py`** - Gestion des feeds
- **`metadata_utils.py`** - Utilitaires de m√©tadonn√©es

### Configuration

- **`requirements.txt`** - D√©pendances Python
- **`data_lake/feeds/active/*.json`** - Configuration des feeds actifs

### Documentation

- **`data_lake/README.md`** - Documentation utilisateur
- **`DESIGN_DOCUMENT.md`** - Ce document

---

## ‚úÖ Checklist de Validation

- [x] Structure de dossiers d√©finie et justifi√©e
- [x] Partitionnement par date pour les streams
- [x] Partitionnement par version pour les tables
- [x] Modes de stockage justifi√©s (APPEND/OVERWRITE)
- [x] Format Parquet avec compression
- [x] Gestion extensible des nouveaux feeds
- [x] M√©tadonn√©es compl√®tes
- [x] Scripts d'export automatis√©s
- [x] Gestion des feeds (add/update/archive)
- [x] Utilitaires de monitoring
- [x] Documentation compl√®te
- [x] S√©curit√© et conformit√© RGPD

---

## üìû Support

Pour toute question ou suggestion d'am√©lioration, consulter:
- Documentation compl√®te: `data_lake/README.md`
- Exemples de feeds: `data_lake/feeds/active/`
- Logs d'export: `data_lake/logs/`

---

**Version**: 1.0  
**Date**: Janvier 2025  
**Auteur**: Data Engineering Team
