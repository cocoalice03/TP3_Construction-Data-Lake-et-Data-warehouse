# üèûÔ∏è Data Lake - Architecture et Documentation

## üìã Vue d'ensemble

Ce data lake stocke toutes les donn√©es provenant de ksqlDB sur le syst√®me de fichiers local avec un partitionnement par date pour optimiser les requ√™tes et la gestion des donn√©es.

## üóÇÔ∏è Structure du Data Lake

```
data_lake/
‚îú‚îÄ‚îÄ streams/                          # Donn√©es des streams ksqlDB
‚îÇ   ‚îú‚îÄ‚îÄ transaction_stream/           # Stream principal (donn√©es brutes)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ year=2025/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ month=01/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ day=15/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_20250115_*.parquet
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ day=16/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ data_20250116_*.parquet
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ month=02/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ _metadata.json
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ transaction_flattened/        # Stream avec sch√©ma aplati
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ year=2025/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ month=01/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ _metadata.json
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ transaction_stream_anonymized/ # Stream anonymis√© + conversion EUR
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ year=2025/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ month=01/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ _metadata.json
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ transaction_stream_blacklisted/ # Transactions blacklist√©es
‚îÇ       ‚îú‚îÄ‚îÄ year=2025/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ month=01/
‚îÇ       ‚îî‚îÄ‚îÄ _metadata.json
‚îÇ
‚îú‚îÄ‚îÄ tables/                           # Donn√©es des tables ksqlDB (agr√©gations)
‚îÇ   ‚îú‚îÄ‚îÄ user_transaction_summary/     # Agr√©gation par utilisateur
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ version=v1/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ snapshot_20250115_120000.parquet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ version=v2/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ snapshot_20250116_120000.parquet
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ _metadata.json
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ user_transaction_summary_eur/ # Agr√©gation en EUR
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ version=v1/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ _metadata.json
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ payment_method_totals/        # Totaux par m√©thode de paiement
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ version=v1/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ _metadata.json
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ product_purchase_counts/      # Compteurs par produit
‚îÇ       ‚îú‚îÄ‚îÄ version=v1/
‚îÇ       ‚îî‚îÄ‚îÄ _metadata.json
‚îÇ
‚îú‚îÄ‚îÄ feeds/                            # Configuration des feeds
‚îÇ   ‚îú‚îÄ‚îÄ active/                       # Feeds actifs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transaction_feed.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ payment_feed.json
‚îÇ   ‚îî‚îÄ‚îÄ archived/                     # Feeds archiv√©s
‚îÇ
‚îî‚îÄ‚îÄ logs/                             # Logs d'export
    ‚îî‚îÄ‚îÄ export_logs_2025_01.log
```

## üìä Streams et Tables ksqlDB

### Streams (Donn√©es en flux continu)

| Stream | Description | Partitionnement | Mode de stockage |
|--------|-------------|-----------------|------------------|
| `transaction_stream` | Stream principal avec donn√©es brutes | Par date (year/month/day) | **APPEND** |
| `transaction_flattened` | Stream avec sch√©ma aplati | Par date (year/month/day) | **APPEND** |
| `transaction_stream_anonymized` | Stream anonymis√© + conversion EUR | Par date (year/month/day) | **APPEND** |
| `transaction_stream_blacklisted` | Transactions des villes blacklist√©es | Par date (year/month/day) | **APPEND** |

### Tables (Agr√©gations mat√©rialis√©es)

| Table | Description | Partitionnement | Mode de stockage |
|-------|-------------|-----------------|------------------|
| `user_transaction_summary` | Montants par utilisateur et type | Par version | **OVERWRITE** |
| `user_transaction_summary_eur` | Montants en EUR par utilisateur | Par version | **OVERWRITE** |
| `payment_method_totals` | Totaux par m√©thode de paiement | Par version | **OVERWRITE** |
| `product_purchase_counts` | Compteurs par produit | Par version | **OVERWRITE** |

## üéØ Justification des Modes de Stockage

### Mode APPEND (Streams)

**Utilis√© pour**: Tous les streams de transactions

**Justification**:
- ‚úÖ Les streams repr√©sentent des **√©v√©nements immuables** dans le temps
- ‚úÖ Chaque transaction est un **√©v√©nement unique** qui ne doit jamais √™tre modifi√©
- ‚úÖ Permet de **conserver l'historique complet** pour audit et tra√ßabilit√©
- ‚úÖ Facilite le **replay** des donn√©es en cas de besoin
- ‚úÖ Optimise les **performances d'√©criture** (pas de recherche/mise √† jour)
- ‚úÖ Compatible avec les principes de **Event Sourcing**

**Exemple**: Une transaction du 15 janvier 2025 sera toujours stock√©e dans `year=2025/month=01/day=15/` et ne sera jamais modifi√©e.

### Mode OVERWRITE (Tables)

**Utilis√© pour**: Toutes les tables d'agr√©gation

**Justification**:
- ‚úÖ Les tables contiennent des **agr√©gations calcul√©es** qui √©voluent dans le temps
- ‚úÖ Chaque export repr√©sente un **snapshot complet** de l'√©tat actuel
- ‚úÖ √âvite la **duplication de donn√©es** agr√©g√©es
- ‚úÖ Simplifie les **requ√™tes analytiques** (toujours la derni√®re version)
- ‚úÖ R√©duit l'**espace disque** utilis√©
- ‚úÖ Facilite la **maintenance** (pas d'accumulation de versions obsol√®tes)

**Exemple**: `user_transaction_summary` est recalcul√© chaque jour avec les totaux √† jour. La version v2 remplace la version v1.

### Mode IGNORE (Non utilis√©)

**Pourquoi pas utilis√©**:
- ‚ùå Ne convient pas aux streams (besoin d'ajouter continuellement)
- ‚ùå Ne convient pas aux tables (besoin de mettre √† jour les agr√©gations)
- ‚ùå Pourrait √™tre utilis√© uniquement pour des **donn√©es de r√©f√©rence statiques** (non pr√©sentes dans ce projet)

## üìÅ Format de Stockage: Parquet

**Choix**: Apache Parquet

**Justifications**:
1. **Compression efficace**: R√©duction de 70-90% de l'espace disque vs JSON
2. **Lecture optimis√©e**: Format columnaire pour requ√™tes analytiques rapides
3. **Sch√©ma int√©gr√©**: Pas besoin de fichier de sch√©ma s√©par√©
4. **Compatible**: Spark, Pandas, DuckDB, Arrow, etc.
5. **Partitionnement**: Support natif du partitionnement par colonnes
6. **Types de donn√©es**: Support complet des types complexes (STRUCT, ARRAY, MAP)

## üîÑ Gestion des Nouveaux Feeds

### Processus d'ajout d'un nouveau feed

1. **Cr√©er la configuration du feed** dans `feeds/active/`
2. **Le script d√©tecte automatiquement** le nouveau feed
3. **Cr√©ation automatique** de la structure de dossiers
4. **Export des donn√©es** selon la configuration

### Configuration d'un feed (JSON)

```json
{
  "feed_name": "payment_feed",
  "feed_type": "stream",
  "ksqldb_source": "payment_stream",
  "description": "Stream des paiements",
  "partitioning": {
    "type": "date",
    "columns": ["year", "month", "day"]
  },
  "storage_mode": "append",
  "format": "parquet",
  "retention_days": 365,
  "enabled": true,
  "created_at": "2025-01-15T10:00:00Z"
}
```

## üìÖ Partitionnement par Date

### Avantages

1. **Performance**: Requ√™tes filtr√©es par date sont ultra-rapides
2. **Maintenance**: Suppression facile des anciennes partitions
3. **Scalabilit√©**: Ajout de nouvelles partitions sans impact
4. **Parall√©lisme**: Lecture/√©criture parall√®le par partition
5. **Co√ªt**: Archivage s√©lectif des anciennes donn√©es

### Exemple de requ√™te optimis√©e

```python
# Lire uniquement les donn√©es de janvier 2025
df = pd.read_parquet(
    'data_lake/streams/transaction_stream_anonymized',
    filters=[('year', '=', 2025), ('month', '=', 1)]
)
```

## üîê S√©curit√© et Conformit√©

### Donn√©es Anonymis√©es

- Les donn√©es sensibles sont stock√©es **uniquement** dans `transaction_stream_anonymized`
- Les donn√©es brutes dans `transaction_stream` doivent √™tre **prot√©g√©es** avec des permissions restrictives
- Les transactions blacklist√©es sont **isol√©es** dans un stream s√©par√©

### Permissions recommand√©es

```bash
# Streams bruts: lecture restreinte
chmod 750 data_lake/streams/transaction_stream/

# Streams anonymis√©s: lecture √©tendue
chmod 755 data_lake/streams/transaction_stream_anonymized/

# Tables: lecture √©tendue
chmod 755 data_lake/tables/
```

## üìä M√©triques et Monitoring

### Fichiers de m√©tadonn√©es (_metadata.json)

Chaque stream/table contient un fichier `_metadata.json` avec:

```json
{
  "source": "transaction_stream",
  "type": "stream",
  "storage_mode": "append",
  "format": "parquet",
  "partitioning": "date",
  "last_export": "2025-01-15T14:30:00Z",
  "total_records": 150000,
  "total_size_mb": 45.2,
  "partitions": [
    {
      "path": "year=2025/month=01/day=15",
      "records": 5000,
      "size_mb": 1.5
    }
  ],
  "schema_version": "1.0",
  "created_at": "2025-01-01T00:00:00Z"
}
```

## üöÄ Utilisation

### Export manuel

```bash
# Exporter tous les streams et tables
python export_to_data_lake.py --all

# Exporter un stream sp√©cifique
python export_to_data_lake.py --stream transaction_stream_anonymized

# Exporter une table sp√©cifique
python export_to_data_lake.py --table user_transaction_summary
```

### Export automatique (cron)

```bash
# Ajouter au crontab pour export quotidien √† 2h du matin
0 2 * * * cd /path/to/project && python export_to_data_lake.py --all >> logs/export.log 2>&1
```

### Ajouter un nouveau feed

```bash
python manage_feeds.py add --name new_feed --type stream --source new_stream
```

## üìà √âvolution Future

### Possibilit√©s d'extension

1. **Compression avanc√©e**: Utiliser Snappy ou ZSTD pour Parquet
2. **Tiering**: D√©placer les anciennes donn√©es vers un stockage froid (S3, Azure Blob)
3. **Indexation**: Cr√©er des index pour acc√©l√©rer les recherches
4. **Catalogue de donn√©es**: Int√©grer avec Apache Hive Metastore ou AWS Glue
5. **Qualit√© des donn√©es**: Ajouter des validations et des checks de qualit√©
6. **Versioning**: Impl√©menter Delta Lake pour le versioning des donn√©es

## üìö Fichiers du Projet

- `export_to_data_lake.py` - Script principal d'export
- `manage_feeds.py` - Gestion des feeds
- `data_lake_config.py` - Configuration centralis√©e
- `README.md` - Cette documentation
