# ğŸ”„ Flux de DonnÃ©es - Data Lake

## Vue d'ensemble du Flux

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SOURCE: KSQLDB                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â”‚ HTTP API (port 8088)
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXTRACTION (KsqlDBClient)                        â”‚
â”‚  â€¢ execute_query("SELECT * FROM stream/table")                     â”‚
â”‚  â€¢ Conversion en DataFrame Pandas                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  TRANSFORMATION (DataLakeExporter)                  â”‚
â”‚  â€¢ DÃ©termination du mode de stockage (APPEND/OVERWRITE)            â”‚
â”‚  â€¢ Calcul du partitionnement (date/version)                        â”‚
â”‚  â€¢ Conversion en format Parquet                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STOCKAGE (File System)                           â”‚
â”‚  â€¢ Ã‰criture des fichiers .parquet                                  â”‚
â”‚  â€¢ Mise Ã  jour des mÃ©tadonnÃ©es                                     â”‚
â”‚  â€¢ Logging des opÃ©rations                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONSOMMATION (Analytics)                         â”‚
â”‚  â€¢ Pandas, DuckDB, Spark                                           â”‚
â”‚  â€¢ RequÃªtes analytiques                                            â”‚
â”‚  â€¢ Visualisations                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Flux DÃ©taillÃ©: Export d'un Stream

### Ã‰tape 1: RequÃªte ksqlDB

```python
# KsqlDBClient.get_stream_data()
query = "SELECT * FROM transaction_stream EMIT CHANGES"
response = requests.post(
    "http://localhost:8088/query",
    json={"ksql": query}
)
```

**DonnÃ©es brutes (JSON)**:
```json
{"row": {"columns": [1, "user123", 100.50, "2025-01-15T10:00:00Z"]}}
{"row": {"columns": [2, "user456", 250.00, "2025-01-15T10:05:00Z"]}}
{"row": {"columns": [3, "user789", 75.25, "2025-01-15T10:10:00Z"]}}
```

### Ã‰tape 2: Conversion en DataFrame

```python
# Conversion en Pandas DataFrame
df = pd.DataFrame(results)
```

**DataFrame**:
```
   id    user_id  amount            timestamp
0   1    user123  100.50  2025-01-15T10:00:00Z
1   2    user456  250.00  2025-01-15T10:05:00Z
2   3    user789   75.25  2025-01-15T10:10:00Z
```

### Ã‰tape 3: Partitionnement

```python
# Calcul du chemin de partition
date = datetime(2025, 1, 15)
partition_path = get_date_partition_path(
    base_path,
    year=2025,
    month=1,
    day=15
)
# â†’ data_lake/streams/transaction_stream/year=2025/month=01/day=15/
```

### Ã‰tape 4: Ã‰criture Parquet

```python
# Conversion en Arrow Table
table = pa.Table.from_pandas(df)

# Ã‰criture avec compression
pq.write_table(
    table,
    file_path,
    compression='snappy',
    use_dictionary=True,
    write_statistics=True
)
```

**Fichier crÃ©Ã©**:
```
data_lake/streams/transaction_stream/
â””â”€â”€ year=2025/month=01/day=15/
    â””â”€â”€ data_20250115_120000.parquet  (15.2 KB)
```

### Ã‰tape 5: MÃ©tadonnÃ©es

```python
# Mise Ã  jour _metadata.json
metadata = {
    "source": "transaction_stream",
    "last_export": "2025-01-15T12:00:00Z",
    "total_records": 3,
    "total_size_mb": 0.015,
    "partitions": [
        {
            "path": "year=2025/month=01/day=15",
            "records": 3,
            "size_mb": 0.015
        }
    ]
}
```

---

## Flux DÃ©taillÃ©: Export d'une Table

### Ã‰tape 1: RequÃªte ksqlDB

```python
# KsqlDBClient.get_table_data()
query = "SELECT * FROM user_transaction_summary"
response = requests.post(
    "http://localhost:8088/query",
    json={"ksql": query}
)
```

**DonnÃ©es (agrÃ©gÃ©es)**:
```json
{"row": {"columns": ["user123", 1500.50, 15]}}
{"row": {"columns": ["user456", 3200.00, 28]}}
{"row": {"columns": ["user789", 850.75, 12]}}
```

### Ã‰tape 2: DÃ©termination de la Version

```python
# Calcul de la prochaine version
current_versions = [1, 2, 3]  # Versions existantes
next_version = max(current_versions) + 1  # â†’ 4
```

### Ã‰tape 3: Partitionnement par Version

```python
partition_path = get_version_partition_path(
    base_path,
    version=4
)
# â†’ data_lake/tables/user_transaction_summary/version=v4/
```

### Ã‰tape 4: Ã‰criture (Mode OVERWRITE)

```python
# Nouvelle version complÃ¨te
pq.write_table(
    table,
    partition_path / f"snapshot_{timestamp}.parquet"
)
```

**Structure crÃ©Ã©e**:
```
data_lake/tables/user_transaction_summary/
â”œâ”€â”€ version=v1/  (ancienne)
â”œâ”€â”€ version=v2/  (ancienne)
â”œâ”€â”€ version=v3/  (ancienne)
â””â”€â”€ version=v4/  (nouvelle)
    â””â”€â”€ snapshot_20250115_120000.parquet
```

### Ã‰tape 5: Nettoyage

```python
# RÃ©tention: 7 versions
# Suppression des versions > 7
if len(versions) > 7:
    for old_version in versions[7:]:
        shutil.rmtree(old_version)
```

**AprÃ¨s nettoyage**:
```
data_lake/tables/user_transaction_summary/
â”œâ”€â”€ version=v4/  (conservÃ©e)
â”œâ”€â”€ version=v3/  (conservÃ©e)
â”œâ”€â”€ version=v2/  (conservÃ©e)
â””â”€â”€ version=v1/  (conservÃ©e si < 7 versions)
```

---

## Comparaison: Stream vs Table

### Stream (APPEND)

```
Jour 1:
â””â”€â”€ year=2025/month=01/day=15/
    â”œâ”€â”€ data_20250115_120000.parquet  (100 records)
    â””â”€â”€ data_20250115_140000.parquet  (150 records)

Jour 2:
â””â”€â”€ year=2025/month=01/day=16/
    â”œâ”€â”€ data_20250116_120000.parquet  (120 records)
    â””â”€â”€ data_20250116_140000.parquet  (180 records)

Total: 550 records (tous conservÃ©s)
```

### Table (OVERWRITE)

```
Export 1:
â””â”€â”€ version=v1/
    â””â”€â”€ snapshot_20250115_120000.parquet  (1000 records)

Export 2:
â””â”€â”€ version=v2/
    â””â”€â”€ snapshot_20250116_120000.parquet  (1050 records)

Export 3:
â””â”€â”€ version=v3/
    â””â”€â”€ snapshot_20250117_120000.parquet  (1100 records)

Total: 3 versions (snapshots complets)
```

---

## Flux de Lecture

### Lecture d'un Stream avec Filtre

```python
# RequÃªte: Transactions de janvier 2025
df = pd.read_parquet(
    'data_lake/streams/transaction_stream',
    filters=[
        ('year', '=', 2025),
        ('month', '=', 1)
    ]
)
```

**Optimisation (Predicate Pushdown)**:
```
1. Scan des partitions disponibles
   âœ“ year=2025/month=01/day=01/
   âœ“ year=2025/month=01/day=02/
   ...
   âœ“ year=2025/month=01/day=31/
   âœ— year=2025/month=02/  (skip)
   âœ— year=2024/          (skip)

2. Lecture uniquement des partitions matchÃ©es
   â†’ Gain de performance significatif
```

### Lecture d'une Table (DerniÃ¨re Version)

```python
# RequÃªte: DerniÃ¨re version de la table
df = pd.read_parquet(
    'data_lake/tables/user_transaction_summary/version=v4'
)
```

**Avantages**:
- Snapshot complet et cohÃ©rent
- Pas besoin d'agrÃ©gation
- Lecture directe

---

## Flux de Gestion des Feeds

### Ajout d'un Nouveau Feed

```
1. CrÃ©ation de la configuration
   â†“
   manage_feeds.py add --name new_stream ...
   â†“
2. GÃ©nÃ©ration du fichier JSON
   â†“
   data_lake/feeds/active/new_stream.json
   â†“
3. CrÃ©ation de la structure
   â†“
   data_lake/streams/new_stream/
   â†“
4. Feed prÃªt pour l'export
   â†“
   export_to_data_lake.py --stream new_stream
```

### Archivage d'un Feed

```
1. Commande d'archivage
   â†“
   manage_feeds.py archive old_stream
   â†“
2. DÃ©placement de la configuration
   â†“
   feeds/active/old_stream.json â†’ feeds/archived/old_stream.json
   â†“
3. DÃ©sactivation du feed
   â†“
   "enabled": false
   â†“
4. Feed archivÃ© (donnÃ©es conservÃ©es)
```

---

## Flux de Monitoring

### GÃ©nÃ©ration de Rapport

```
1. Lecture des mÃ©tadonnÃ©es
   â†“
   MetadataReader.list_all_metadata()
   â†“
2. AgrÃ©gation des statistiques
   â†“
   â€¢ Total records
   â€¢ Total size
   â€¢ Last exports
   â†“
3. GÃ©nÃ©ration du rapport
   â†“
   MetadataAnalyzer.generate_report()
   â†“
4. Affichage ou export
   â†“
   Console / CSV / JSON
```

---

## Chronologie d'un Export Complet

```
T+0s    : DÃ©marrage export_to_data_lake.py --all
T+1s    : Connexion Ã  ksqlDB
T+2s    : RÃ©cupÃ©ration transaction_stream (1000 records)
T+3s    : Partitionnement par date
T+4s    : Ã‰criture Parquet (compression)
T+5s    : Mise Ã  jour mÃ©tadonnÃ©es
T+6s    : RÃ©cupÃ©ration transaction_flattened (1000 records)
...
T+30s   : RÃ©cupÃ©ration user_transaction_summary (50 records)
T+31s   : Partitionnement par version
T+32s   : Ã‰criture Parquet
T+33s   : Nettoyage anciennes versions
T+34s   : Mise Ã  jour mÃ©tadonnÃ©es
...
T+60s   : Export terminÃ©
          âœ“ 4 streams exportÃ©s
          âœ“ 4 tables exportÃ©es
          âœ“ 8 fichiers de mÃ©tadonnÃ©es mis Ã  jour
```

---

## Flux d'Erreur et RÃ©cupÃ©ration

### Erreur de Connexion ksqlDB

```
1. Tentative de connexion
   â†“
   KsqlDBClient(host, port)
   â†“
2. Timeout ou erreur
   â†“
   requests.exceptions.ConnectionError
   â†“
3. Log de l'erreur
   â†“
   logger.error("Impossible de se connecter Ã  ksqlDB")
   â†“
4. ArrÃªt gracieux
   â†“
   sys.exit(1)
```

### Erreur d'Ã‰criture

```
1. Tentative d'Ã©criture Parquet
   â†“
   pq.write_table(table, file_path)
   â†“
2. Erreur (permissions, espace disque)
   â†“
   OSError / IOError
   â†“
3. Rollback
   â†“
   â€¢ Suppression fichier partiel
   â€¢ Pas de mise Ã  jour mÃ©tadonnÃ©es
   â†“
4. Log et rapport d'erreur
   â†“
   logger.error("Erreur lors de l'Ã©criture")
```

---

## Optimisations du Flux

### 1. Batch Processing

```python
# Traitement par lots pour grandes tables
for batch in pd.read_sql(query, chunksize=10000):
    process_batch(batch)
```

### 2. Compression Intelligente

```python
# Snappy: rapide, bon compromis
# ZSTD: meilleure compression, plus lent
compression = 'snappy' if speed_priority else 'zstd'
```

### 3. ParallÃ©lisation

```python
# Export parallÃ¨le de plusieurs streams
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=4) as executor:
    futures = [
        executor.submit(export_stream, stream)
        for stream in streams
    ]
```

---

## MÃ©triques de Performance

### Temps d'Export Typiques

| Type | Records | Taille | Temps |
|------|---------|--------|-------|
| Stream | 10,000 | 5 MB | 2-3s |
| Stream | 100,000 | 50 MB | 15-20s |
| Table | 1,000 | 500 KB | 1-2s |
| Table | 10,000 | 5 MB | 3-5s |

### Taux de Compression

| Format | Taille Originale | Taille Parquet | Ratio |
|--------|------------------|----------------|-------|
| JSON | 100 MB | 15 MB | 85% |
| CSV | 80 MB | 12 MB | 85% |

---

**Version**: 1.0  
**DerniÃ¨re mise Ã  jour**: Janvier 2025
