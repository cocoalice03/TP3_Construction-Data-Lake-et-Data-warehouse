"""
Kafka Consumer pour le Data Lake (Parquet)
Consomme les messages Kafka et les √©crit dans le Data Lake au format Parquet
"""
import json
import logging
import sys
from datetime import datetime, date
from pathlib import Path
from typing import List, Dict, Any
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from kafka import KafkaConsumer
from kafka.errors import KafkaError

from kafka_config import (
    KAFKA_CONFIG, KAFKA_TOPICS, BATCH_CONFIG,
    DATA_LAKE_ROOT, LOGS_DIR, LOG_FORMAT, LOG_LEVEL,
    get_topics_for_destination, get_topic_config
)
from data_lake_config import (
    STREAMS_DIR, TABLES_DIR, PARQUET_COMPRESSION,
    get_date_partition_path, get_version_partition_path,
    ensure_directories
)


# Configuration du logging
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL),
    format=LOG_FORMAT,
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(
            LOGS_DIR / f"kafka_consumer_datalake_{datetime.now().strftime('%Y%m')}.log"
        )
    ]
)
logger = logging.getLogger(__name__)


class DataLakeKafkaConsumer:
    """Consumer Kafka pour le Data Lake"""
    
    def __init__(self, topics: List[str] = None):
        """
        Initialise le consumer Kafka pour le Data Lake
        
        Args:
            topics: Liste des topics √† consommer (None = tous les topics data_lake)
        """
        ensure_directories()
        
        # D√©terminer les topics √† consommer
        if topics is None:
            self.topics = get_topics_for_destination("data_lake")
        else:
            self.topics = topics
        
        logger.info(f"Initialisation du consumer pour les topics: {self.topics}")
        
        # Cr√©er le consumer Kafka
        self.consumer = KafkaConsumer(
            *self.topics,
            bootstrap_servers=KAFKA_CONFIG["bootstrap_servers"],
            group_id=KAFKA_CONFIG["group_id"],
            auto_offset_reset=KAFKA_CONFIG["auto_offset_reset"],
            enable_auto_commit=KAFKA_CONFIG["enable_auto_commit"],
            auto_commit_interval_ms=KAFKA_CONFIG["auto_commit_interval_ms"],
            session_timeout_ms=KAFKA_CONFIG["session_timeout_ms"],
            max_poll_records=KAFKA_CONFIG["max_poll_records"],
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            key_deserializer=lambda m: m.decode('utf-8') if m else None
        )
        
        # Buffers pour le batch processing
        self.message_buffers = {topic: [] for topic in self.topics}
        self.last_flush_time = {topic: datetime.now() for topic in self.topics}
        
        logger.info("‚úì Consumer Kafka initialis√©")
    
    def consume(self):
        """Consomme les messages Kafka en continu"""
        logger.info("üöÄ D√©marrage de la consommation des messages Kafka")
        
        try:
            for message in self.consumer:
                try:
                    self.process_message(message)
                except Exception as e:
                    logger.error(f"Erreur lors du traitement du message: {e}")
                    logger.error(f"Topic: {message.topic}, Partition: {message.partition}, Offset: {message.offset}")
        
        except KeyboardInterrupt:
            logger.info("Arr√™t demand√© par l'utilisateur")
        
        except KafkaError as e:
            logger.error(f"Erreur Kafka: {e}")
        
        finally:
            self.flush_all_buffers()
            self.consumer.close()
            logger.info("Consumer Kafka ferm√©")
    
    def process_message(self, message):
        """Traite un message Kafka"""
        topic = message.topic
        value = message.value
        
        # Ajouter le message au buffer
        self.message_buffers[topic].append(value)
        
        # V√©rifier si on doit flush le buffer
        should_flush = (
            len(self.message_buffers[topic]) >= BATCH_CONFIG["batch_size"] or
            (datetime.now() - self.last_flush_time[topic]).total_seconds() >= BATCH_CONFIG["batch_timeout_seconds"]
        )
        
        if should_flush:
            self.flush_buffer(topic)
    
    def flush_buffer(self, topic: str):
        """√âcrit le buffer d'un topic dans le Data Lake"""
        if not self.message_buffers[topic]:
            return
        
        messages = self.message_buffers[topic]
        logger.info(f"Flush de {len(messages)} messages pour le topic {topic}")
        
        try:
            # R√©cup√©rer la configuration du topic
            topic_config = get_topic_config(topic)
            if not topic_config:
                logger.warning(f"Configuration non trouv√©e pour le topic {topic}")
                return
            
            # Convertir en DataFrame
            df = pd.DataFrame(messages)
            
            if df.empty:
                logger.warning(f"DataFrame vide pour le topic {topic}")
                return
            
            # D√©terminer le chemin de destination
            if topic_config["feed_type"] == "stream":
                self.write_stream_data(topic, df, topic_config)
            else:
                self.write_table_data(topic, df, topic_config)
            
            # Vider le buffer
            self.message_buffers[topic] = []
            self.last_flush_time[topic] = datetime.now()
            
            logger.info(f"‚úì {len(messages)} messages √©crits pour {topic}")
        
        except Exception as e:
            logger.error(f"Erreur lors du flush du buffer pour {topic}: {e}")
            # En cas d'erreur, on garde les messages dans le buffer pour retry
    
    def write_stream_data(self, topic: str, df: pd.DataFrame, config: dict):
        """√âcrit les donn√©es d'un stream dans le Data Lake"""
        # Partitionnement par date
        today = date.today()
        partition_path = get_date_partition_path(
            STREAMS_DIR / topic,
            year=today.year,
            month=today.month,
            day=today.day
        )
        
        # Cr√©er le dossier si n√©cessaire
        partition_path.mkdir(parents=True, exist_ok=True)
        
        # Nom du fichier avec timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_path = partition_path / f"data_{timestamp}.parquet"
        
        # Convertir en Arrow Table et √©crire
        table = pa.Table.from_pandas(df)
        pq.write_table(
            table,
            file_path,
            compression=PARQUET_COMPRESSION,
            use_dictionary=True,
            write_statistics=True
        )
        
        logger.info(f"‚úì Stream {topic} √©crit: {file_path}")
    
    def write_table_data(self, topic: str, df: pd.DataFrame, config: dict):
        """√âcrit les donn√©es d'une table dans le Data Lake"""
        # D√©terminer la prochaine version
        base_path = TABLES_DIR / topic
        base_path.mkdir(parents=True, exist_ok=True)
        
        # Trouver les versions existantes
        existing_versions = []
        for version_dir in base_path.glob("version=v*"):
            try:
                version_num = int(version_dir.name.replace("version=v", ""))
                existing_versions.append(version_num)
            except ValueError:
                continue
        
        next_version = max(existing_versions, default=0) + 1
        
        # Cr√©er le chemin de partition
        partition_path = get_version_partition_path(base_path, version=next_version)
        partition_path.mkdir(parents=True, exist_ok=True)
        
        # Nom du fichier avec timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_path = partition_path / f"snapshot_{timestamp}.parquet"
        
        # Convertir en Arrow Table et √©crire
        table = pa.Table.from_pandas(df)
        pq.write_table(
            table,
            file_path,
            compression=PARQUET_COMPRESSION,
            use_dictionary=True,
            write_statistics=True
        )
        
        logger.info(f"‚úì Table {topic} √©crite: {file_path} (version {next_version})")
        
        # Nettoyage des anciennes versions si n√©cessaire
        self.cleanup_old_versions(base_path, retention_versions=7)
    
    def cleanup_old_versions(self, base_path: Path, retention_versions: int = 7):
        """Supprime les anciennes versions au-del√† de la r√©tention"""
        version_dirs = sorted(
            base_path.glob("version=v*"),
            key=lambda p: int(p.name.replace("version=v", ""))
        )
        
        if len(version_dirs) > retention_versions:
            for old_version in version_dirs[:-retention_versions]:
                logger.info(f"Suppression de l'ancienne version: {old_version}")
                import shutil
                shutil.rmtree(old_version)
    
    def flush_all_buffers(self):
        """Flush tous les buffers avant fermeture"""
        logger.info("Flush de tous les buffers...")
        for topic in self.topics:
            self.flush_buffer(topic)


def main():
    """Point d'entr√©e principal"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Kafka Consumer pour le Data Lake"
    )
    parser.add_argument(
        "--topics",
        type=str,
        nargs="+",
        help="Topics Kafka √† consommer (optionnel)"
    )
    
    args = parser.parse_args()
    
    try:
        consumer = DataLakeKafkaConsumer(topics=args.topics)
        consumer.consume()
    
    except Exception as e:
        logger.error(f"Erreur fatale: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
