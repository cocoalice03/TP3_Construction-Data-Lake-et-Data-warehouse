# üîÑ Guide des Kafka Consumers

## üìã Vue d'ensemble

Ce guide d√©crit les applications Kafka consumers qui peuplent automatiquement le Data Lake et le Data Warehouse en temps r√©el.

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      KAFKA TOPICS                            ‚îÇ
‚îÇ  ‚Ä¢ transaction_stream                                        ‚îÇ
‚îÇ  ‚Ä¢ transaction_flattened                                     ‚îÇ
‚îÇ  ‚Ä¢ transaction_stream_anonymized                             ‚îÇ
‚îÇ  ‚Ä¢ user_transaction_summary                                  ‚îÇ
‚îÇ  ‚Ä¢ payment_method_totals                                     ‚îÇ
‚îÇ  ‚Ä¢ product_purchase_counts                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                         ‚îÇ
        ‚ñº                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CONSUMER         ‚îÇ     ‚îÇ  CONSUMER            ‚îÇ
‚îÇ  DATA LAKE        ‚îÇ     ‚îÇ  DATA WAREHOUSE      ‚îÇ
‚îÇ  (Parquet)        ‚îÇ     ‚îÇ  (MySQL)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                           ‚îÇ
         ‚ñº                           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DATA LAKE        ‚îÇ     ‚îÇ  DATA WAREHOUSE      ‚îÇ
‚îÇ  ‚Ä¢ Streams        ‚îÇ     ‚îÇ  ‚Ä¢ dim_users         ‚îÇ
‚îÇ  ‚Ä¢ Tables         ‚îÇ     ‚îÇ  ‚Ä¢ dim_payment...    ‚îÇ
‚îÇ  ‚Ä¢ Parquet files  ‚îÇ     ‚îÇ  ‚Ä¢ fact_user...      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üì¶ Composants

### 1. kafka_config.py
**Configuration centralis√©e** pour tous les consumers

- Configuration Kafka (bootstrap servers, group ID, etc.)
- Mapping des topics vers destinations
- Configuration du batch processing
- Configuration MySQL

### 2. kafka_consumer_datalake.py
**Consumer pour le Data Lake** (Parquet)

- Consomme les topics Kafka (streams et tables)
- √âcrit les donn√©es au format Parquet
- Partitionnement automatique (date/version)
- Batch processing pour performance

### 3. kafka_consumer_warehouse.py
**Consumer pour le Data Warehouse** (MySQL)

- Consomme uniquement les topics tables
- Ins√®re les donn√©es dans MySQL
- Gestion des dimensions (users, payment_methods)
- Snapshots versionn√©s

### 4. kafka_consumer_orchestrator.py
**Orchestrateur** pour g√©rer les consumers

- Lance les consumers en parall√®le
- Gestion des processus
- Arr√™t gracieux
- Monitoring

---

## üöÄ Installation

### 1. Installer les D√©pendances

```bash
# Activer l'environnement virtuel
source venv/bin/activate

# Installer Kafka
pip install kafka-python>=2.0.2

# Ou r√©installer toutes les d√©pendances
pip install -r requirements.txt
```

### 2. V√©rifier Kafka

```bash
# V√©rifier que Kafka est accessible
nc -zv localhost 9092

# Lister les topics
kafka-topics --bootstrap-server localhost:9092 --list
```

---

## ‚öôÔ∏è Configuration

### √âditer kafka_config.py

```python
# Configuration Kafka
KAFKA_CONFIG = {
    "bootstrap_servers": ["localhost:9092"],  # Votre serveur Kafka
    "group_id": "data_lake_consumers",
    "auto_offset_reset": "earliest",  # ou 'latest'
}

# Configuration MySQL
MYSQL_CONFIG = {
    "host": "localhost",
    "port": 3306,
    "database": "data_warehouse",
    "user": "root",
    "password": "",  # √Ä configurer
}
```

### Topics Configur√©s

**Streams** (Data Lake uniquement):
- `transaction_stream`
- `transaction_flattened`
- `transaction_stream_anonymized`
- `transaction_stream_blacklisted`

**Tables** (Data Lake + Data Warehouse):
- `user_transaction_summary`
- `user_transaction_summary_eur`
- `payment_method_totals`
- `product_purchase_counts`

---

## üéØ Utilisation

### Option 1: Orchestrateur (Recommand√©)

Lance tous les consumers en parall√®le:

```bash
# Activer l'environnement virtuel
source venv/bin/activate

# Lancer tous les consumers
python kafka_consumer_orchestrator.py \
  --mode all \
  --mysql-password votre_mot_de_passe

# Lancer uniquement Data Lake
python kafka_consumer_orchestrator.py --mode datalake

# Lancer uniquement Data Warehouse
python kafka_consumer_orchestrator.py \
  --mode warehouse \
  --mysql-password votre_mot_de_passe
```

### Option 2: Consumers Individuels

#### Consumer Data Lake

```bash
# Tous les topics
python kafka_consumer_datalake.py

# Topics sp√©cifiques
python kafka_consumer_datalake.py \
  --topics transaction_stream user_transaction_summary
```

#### Consumer Data Warehouse

```bash
# Tous les topics tables
python kafka_consumer_warehouse.py \
  --mysql-password votre_mot_de_passe

# Topics sp√©cifiques
python kafka_consumer_warehouse.py \
  --topics user_transaction_summary payment_method_totals \
  --mysql-password votre_mot_de_passe
```

---

## üîÑ Fonctionnement

### Batch Processing

Les consumers utilisent le **batch processing** pour optimiser les performances:

1. **Accumulation**: Les messages sont accumul√©s dans un buffer
2. **Flush**: Le buffer est vid√© quand:
   - Taille du batch atteinte (1000 messages par d√©faut)
   - Timeout atteint (30 secondes par d√©faut)
3. **√âcriture**: Les donn√©es sont √©crites en une seule op√©ration

### Partitionnement Automatique

#### Streams (Data Lake)
```
data_lake/streams/transaction_stream/
‚îî‚îÄ‚îÄ year=2025/month=01/day=28/
    ‚îú‚îÄ‚îÄ data_20250128_140000.parquet
    ‚îú‚îÄ‚îÄ data_20250128_140030.parquet
    ‚îî‚îÄ‚îÄ data_20250128_140100.parquet
```

#### Tables (Data Lake)
```
data_lake/tables/user_transaction_summary/
‚îú‚îÄ‚îÄ version=v1/
‚îÇ   ‚îî‚îÄ‚îÄ snapshot_20250128_140000.parquet
‚îú‚îÄ‚îÄ version=v2/
‚îÇ   ‚îî‚îÄ‚îÄ snapshot_20250128_150000.parquet
‚îî‚îÄ‚îÄ version=v3/
    ‚îî‚îÄ‚îÄ snapshot_20250128_160000.parquet
```

### Insertion MySQL

Les tables sont ins√©r√©es avec **UPSERT** (INSERT ... ON DUPLICATE KEY UPDATE):
- √âvite les doublons
- Met √† jour les donn√©es existantes
- G√®re les snapshots versionn√©s

---

## üìä Monitoring

### Logs

```bash
# Logs de l'orchestrateur
tail -f data_lake/logs/kafka_orchestrator_*.log

# Logs du consumer Data Lake
tail -f data_lake/logs/kafka_consumer_datalake_*.log

# Logs du consumer Data Warehouse
tail -f data_lake/logs/kafka_consumer_warehouse_*.log
```

### V√©rifier les Donn√©es

#### Data Lake

```bash
# Lister les fichiers cr√©√©s
find data_lake/streams -name "*.parquet" -mtime -1

# Lire un fichier Parquet
python -c "import pandas as pd; df = pd.read_parquet('data_lake/streams/transaction_stream/year=2025/month=01/day=28/data_*.parquet'); print(df.head())"
```

#### Data Warehouse

```bash
# V√©rifier les insertions
mysql -u root -p -e "
USE data_warehouse;
SELECT 
    'fact_user_transaction_summary' as table_name,
    COUNT(*) as count,
    MAX(created_at) as last_insert
FROM fact_user_transaction_summary
UNION ALL
SELECT 
    'fact_payment_method_totals',
    COUNT(*),
    MAX(created_at)
FROM fact_payment_method_totals;
"
```

---

## üîß Configuration Avanc√©e

### Ajuster le Batch Processing

√âditer `kafka_config.py`:

```python
BATCH_CONFIG = {
    "batch_size": 1000,  # Augmenter pour plus de performance
    "batch_timeout_seconds": 30,  # R√©duire pour plus de r√©activit√©
    "max_retries": 3,
    "retry_delay_seconds": 5
}
```

### Ajuster la Configuration Kafka

```python
KAFKA_CONFIG = {
    "bootstrap_servers": ["localhost:9092"],
    "group_id": "data_lake_consumers",
    "auto_offset_reset": "earliest",  # 'earliest' ou 'latest'
    "enable_auto_commit": True,
    "auto_commit_interval_ms": 5000,  # Fr√©quence de commit
    "session_timeout_ms": 30000,
    "max_poll_records": 500,  # Messages par poll
    "max_poll_interval_ms": 300000,
}
```

### Ajouter un Nouveau Topic

1. √âditer `kafka_config.py`:

```python
KAFKA_TOPICS = {
    "streams": [
        # ... topics existants ...
        {
            "topic": "nouveau_stream",
            "feed_type": "stream",
            "destination": "data_lake",
            "partitioning": "date",
            "storage_mode": "append",
            "enabled": True
        }
    ]
}
```

2. Red√©marrer les consumers

---

## üêõ Troubleshooting

### Erreur de Connexion Kafka

```bash
# V√©rifier que Kafka est d√©marr√©
ps aux | grep kafka

# Tester la connexion
nc -zv localhost 9092

# V√©rifier les topics
kafka-topics --bootstrap-server localhost:9092 --list
```

### Consumer Lag

```bash
# V√©rifier le lag des consumers
kafka-consumer-groups --bootstrap-server localhost:9092 \
  --group data_lake_consumers \
  --describe
```

### Erreur MySQL

```bash
# V√©rifier la connexion
mysql -u root -p -e "SELECT 1;"

# V√©rifier les permissions
mysql -u root -p -e "SHOW GRANTS;"
```

### Messages Non Consomm√©s

```bash
# V√©rifier les offsets
kafka-consumer-groups --bootstrap-server localhost:9092 \
  --group data_lake_consumers \
  --describe

# R√©initialiser les offsets (ATTENTION: perte de donn√©es)
kafka-consumer-groups --bootstrap-server localhost:9092 \
  --group data_lake_consumers \
  --reset-offsets --to-earliest \
  --all-topics \
  --execute
```

---

## üîÑ Automatisation

### Systemd Service (Linux)

Cr√©er `/etc/systemd/system/kafka-consumers.service`:

```ini
[Unit]
Description=Kafka Consumers for Data Lake and Data Warehouse
After=network.target kafka.service mysql.service

[Service]
Type=simple
User=alice
WorkingDirectory=/Users/alice/Downloads/data_lake
Environment="PATH=/Users/alice/Downloads/data_lake/venv/bin"
ExecStart=/Users/alice/Downloads/data_lake/venv/bin/python kafka_consumer_orchestrator.py --mode all --mysql-password PASSWORD
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target
```

```bash
# Activer et d√©marrer
sudo systemctl enable kafka-consumers
sudo systemctl start kafka-consumers

# V√©rifier le statut
sudo systemctl status kafka-consumers
```

### Launchd (macOS)

Cr√©er `~/Library/LaunchAgents/com.datalake.kafka-consumers.plist`:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>Label</key>
    <string>com.datalake.kafka-consumers</string>
    <key>ProgramArguments</key>
    <array>
        <string>/Users/alice/Downloads/data_lake/venv/bin/python</string>
        <string>/Users/alice/Downloads/data_lake/kafka_consumer_orchestrator.py</string>
        <string>--mode</string>
        <string>all</string>
        <string>--mysql-password</string>
        <string>PASSWORD</string>
    </array>
    <key>WorkingDirectory</key>
    <string>/Users/alice/Downloads/data_lake</string>
    <key>RunAtLoad</key>
    <true/>
    <key>KeepAlive</key>
    <true/>
    <key>StandardOutPath</key>
    <string>/Users/alice/Downloads/data_lake/data_lake/logs/kafka_consumers.log</string>
    <key>StandardErrorPath</key>
    <string>/Users/alice/Downloads/data_lake/data_lake/logs/kafka_consumers_error.log</string>
</dict>
</plist>
```

```bash
# Charger le service
launchctl load ~/Library/LaunchAgents/com.datalake.kafka-consumers.plist

# D√©marrer
launchctl start com.datalake.kafka-consumers

# V√©rifier
launchctl list | grep kafka-consumers
```

---

## üìà Performance

### M√©triques Typiques

| M√©trique | Valeur |
|----------|--------|
| **Throughput** | 1000-5000 msg/s par consumer |
| **Latence** | < 100ms (batch processing) |
| **Batch size** | 1000 messages |
| **Batch timeout** | 30 secondes |
| **Compression Parquet** | 70-90% |

### Optimisations

1. **Augmenter le batch size** pour plus de throughput
2. **R√©duire le batch timeout** pour moins de latence
3. **Parall√©liser** avec plusieurs partitions Kafka
4. **Utiliser SSD** pour le Data Lake
5. **Optimiser MySQL** (index, buffer pool)

---

## ‚úÖ Checklist de Validation

### Installation
- [ ] Kafka install√© et d√©marr√©
- [ ] Topics Kafka cr√©√©s
- [ ] D√©pendances Python install√©es (`kafka-python`)
- [ ] Configuration √©dit√©e (`kafka_config.py`)

### Tests
- [ ] Consumer Data Lake d√©marre sans erreur
- [ ] Consumer Data Warehouse d√©marre sans erreur
- [ ] Fichiers Parquet cr√©√©s dans `data_lake/`
- [ ] Donn√©es ins√©r√©es dans MySQL
- [ ] Logs sans erreur

### Production
- [ ] Service systemd/launchd configur√©
- [ ] Monitoring en place
- [ ] Alertes configur√©es
- [ ] Backup automatique

---

## üìö Documentation Compl√®te

- **kafka_config.py** - Configuration centralis√©e
- **kafka_consumer_datalake.py** - Consumer Data Lake
- **kafka_consumer_warehouse.py** - Consumer Data Warehouse
- **kafka_consumer_orchestrator.py** - Orchestrateur

---

**Version**: 1.0  
**Date**: Janvier 2025  
**Statut**: ‚úÖ Pr√™t pour la Production
